import json
import random
import os
import uuid
from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser

# ================= é…ç½® =================
OUTPUT_FILE = "experiment_dataset_hard.json"
NUM_SAMPLES = 20        # æ€»æ ·æœ¬æ•°
DISTRACTOR_MSG_COUNT = 8 # å¹²æ‰°æ¶ˆæ¯æ•°é‡ (å»ºè®® 8-12)
DISTRACTOR_LEN = 150     # å¹²æ‰°æ¶ˆæ¯é•¿åº¦

# LLM åˆå§‹åŒ–
llm = ChatOpenAI(
    model="qwen-plus", # å»ºè®®ç”¨å¼ºåŠ›æ¨¡å‹ç”Ÿæˆæ•°æ®ï¼Œå¦‚ qwen-max æˆ– gpt-4
    temperature=0.8,
    openai_api_key="sk-2770a3f619c14f31a87d47924de34af2",
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# è¯»å–èƒŒæ™¯å™ªéŸ³
if not os.path.exists("background.txt"):
    BACKGROUND_TEXT = "äººå·¥æ™ºèƒ½å‘å±•è¿…é€Ÿï¼Œæ·±åº¦å­¦ä¹ æ˜¯å…¶ä¸­çš„æ ¸å¿ƒæŠ€æœ¯..." * 100
else:
    with open("background.txt", "r", encoding="utf-8") as f:
        BACKGROUND_TEXT = f.read()

def get_noise(length=100):
    """è·å–çº¯èƒŒæ™¯å™ªéŸ³"""
    if len(BACKGROUND_TEXT) < length: return BACKGROUND_TEXT
    start = random.randint(0, len(BACKGROUND_TEXT) - length - 10)
    return BACKGROUND_TEXT[start:start+length].replace("\n", " ")

# ================= æ•°æ®ç»“æ„å®šä¹‰ =================

class MultiHopCase(BaseModel):
    """å¤šè·³æ¨ç†ç±»å‹ï¼šéœ€è¦ç»“åˆä¸¤ä¸ªäº‹å®"""
    fact_1: str = Field(description="ç¬¬ä¸€ä¸ªäº‹å®ï¼Œä¾‹å¦‚ï¼š'Aæ˜¯Bçš„çˆ¶äº²'")
    fact_2: str = Field(description="ç¬¬äºŒä¸ªäº‹å®ï¼Œä¾‹å¦‚ï¼š'Bæ˜¯Cçš„è€å¸ˆ'")
    question: str = Field(description="éœ€è¦ç»“åˆä¸¤è€…çš„æé—®ï¼Œä¾‹å¦‚ï¼š'Açš„å­©å­ä»äº‹ä»€ä¹ˆèŒä¸šï¼Ÿ'")
    answer: str = Field(description="æ ‡å‡†ç­”æ¡ˆ")

class AdversarialCase(BaseModel):
    """å¯¹æŠ—å¹²æ‰°ç±»å‹ï¼šåŒ…å«äº‹å®å’Œæ··æ·†é¡¹"""
    true_fact: str = Field(description="çœŸå®çš„äº‹å®ï¼Œä¾‹å¦‚ï¼š'å¯†ç æ˜¯1234'")
    fake_fact: str = Field(description="å¹²æ‰°æ€§æå¼ºçš„äº‹å®ï¼Œä¾‹å¦‚ï¼š'æ—§å¯†ç æ˜¯1234ä½†å·²è¿‡æœŸ' æˆ– 'ç®¡ç†å‘˜çš„IDæ˜¯1234'")
    question: str = Field(description="æé—®")
    answer: str = Field(description="æ ‡å‡†ç­”æ¡ˆ")

# ================= ç”Ÿæˆé€»è¾‘ =================

def generate_dataset_hard():
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆé«˜éš¾åº¦å¯¹æŠ—æ•°æ®é›† (å…± {NUM_SAMPLES} æ¡)...")
    dataset = []

    # 1. å®šä¹‰ Prompt æ¨¡æ¿
    
    # å¤šè·³ Prompt
    prompt_multi = ChatPromptTemplate.from_template("""
    è¯·ç”Ÿæˆä¸€ä¸ªã€å¤šè·³æ¨ç†ã€‘æµ‹è¯•ç”¨ä¾‹ã€‚
    è¦æ±‚ï¼š
    1. äº‹å®å¿…é¡»æ˜¯è™šæ„çš„ï¼ˆç§‘å¹»/é­”å¹»/è°æˆ˜èƒŒæ™¯ï¼‰ã€‚
    2. ç­”æ¡ˆå¿…é¡»ä¾èµ–ä¸¤ä¸ªäº‹å®æ‰èƒ½æ¨å¯¼å‡ºæ¥ï¼Œç¼ºä¸€ä¸å¯ã€‚
    3. ä¸¤ä¸ªäº‹å®ä¸è¦åœ¨è¯­ä¹‰ä¸Šè¿‡äºæ¥è¿‘ï¼Œæœ€å¥½æ¶‰åŠä¸åŒçš„äººç‰©æˆ–åœ°ç‚¹ã€‚
    
    {format_instructions}
    """)
    
    # å¯¹æŠ— Prompt
    prompt_adv = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä¸ªä¸“æ”»ã€å¤§æ¨¡å‹å¯¹æŠ—æ”»å‡»ã€‘çš„æ•°æ®é›†ç”Ÿæˆä¸“å®¶ã€‚ä½ éœ€è¦ç”Ÿæˆä¸€ç»„éå¸¸éš¾ä»¥åŒºåˆ†çš„â€œäº‹å® vs å¹²æ‰°â€æ•°æ®ã€‚

    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ç”Ÿæˆï¼š

    Step 1: ç¡®å®šä¸€ä¸ªã€æ ¸å¿ƒå®ä½“ã€‘ï¼ˆå¦‚æŸä¸ªäººåã€åœ°ç‚¹ã€è®¡åˆ’ä»£å·ï¼‰ã€‚
    Step 2: è®¾è®¡ä¸€ä¸ªã€çœŸå®äº‹å® (true_fact)ã€‘ï¼Œæè¿°è¯¥å®ä½“çš„å½“å‰çŠ¶æ€ã€‚
    Step 3: è®¾è®¡ä¸€ä¸ªã€å¹²æ‰°äº‹å® (fake_fact)ã€‘ã€‚è¦æ±‚ï¼š
        - å¿…é¡»åŒ…å«ã€æ ¸å¿ƒå®ä½“ã€‘çš„åç§°ï¼ˆç¡®ä¿å‘é‡ç›¸ä¼¼åº¦æé«˜ï¼‰ã€‚
        - å¿…é¡»ä¸çœŸå®äº‹å®åœ¨è¯­ä¹‰ä¸Šå†²çªã€‚
        - é‡‡ç”¨ä»¥ä¸‹ä¸‰ç§æ”»å‡»æ¨¡å¼ä¹‹ä¸€ï¼š
            A. ã€æ—¶åºè¿‡æœŸæ¨¡å¼ã€‘: å¹²æ‰°äº‹å®æ˜¯â€œæ—§çš„/è¿‡æœŸçš„â€ä¿¡æ¯ã€‚
            (ä¾‹: çœŸ="å¯†ç ç°åœ¨æ˜¯999"; å‡="å¯†ç ä¸Šå‘¨è¿˜æ˜¯000")
            B. ã€å¦å®š/å–æ¶ˆæ¨¡å¼ã€‘: å¹²æ‰°äº‹å®æ˜¯â€œè¢«å¦å†³/å–æ¶ˆâ€çš„è®¡åˆ’ã€‚
            (ä¾‹: çœŸ="æˆ‘ä»¬æœ€ç»ˆé€‰æ‹©äº†Bæ–¹æ¡ˆ"; å‡="Aæ–¹æ¡ˆåŸæœ¬æ˜¯é¦–é€‰ä½†è¢«åºŸå¼ƒäº†")
            C. ã€ä¸»ä½“æ··æ·†æ¨¡å¼ã€‘: æè¿°æå…¶ç›¸ä¼¼çš„å¦ä¸€ä¸ªäººçš„çŠ¶æ€ã€‚
            (ä¾‹: çœŸ="ç‰¹å·¥007çš„ä»£å·æ˜¯é¹°"; å‡="ç‰¹å·¥006çš„ä»£å·æ˜¯é¹°")

    Step 4: åŸºäºã€çœŸå®äº‹å®ã€‘ç”Ÿæˆé—®é¢˜ã€‚

    ---
    ã€Few-Shot ç¤ºä¾‹ã€‘:
    1. 
    true_fact: "è“å®çŸ³å·é£èˆ¹çš„å‘å°„ä»£ç æ˜¯ Alpha-9ã€‚"
    fake_fact: "è“å®çŸ³å·é£èˆ¹åŸæœ¬çš„é¢„è®¾ä»£ç æ˜¯ Beta-1ï¼Œä½†åæ¥åºŸå¼ƒäº†ã€‚"
    question: "è“å®çŸ³å·é£èˆ¹çš„æœ€ç»ˆå‘å°„ä»£ç æ˜¯ä»€ä¹ˆï¼Ÿ"
    answer: "Alpha-9"

    2.
    true_fact: "ç°ä»»è´¢åŠ¡ä¸»ç®¡æ˜¯ Sarah Connorã€‚"
    fake_fact: "John Connor æ›¾æ‹…ä»»è´¢åŠ¡ä¸»ç®¡ï¼Œä½†ä»–ä¸Šä¸ªæœˆç¦»èŒäº†ã€‚"
    question: "ç°åœ¨çš„è´¢åŠ¡ä¸»ç®¡æ˜¯è°ï¼Ÿ"
    answer: "Sarah Connor"
    ---

    è¯·è¾“å‡º JSON æ ¼å¼:
    {format_instructions}
    """)

    parser_multi = PydanticOutputParser(pydantic_object=MultiHopCase)
    parser_adv = PydanticOutputParser(pydantic_object=AdversarialCase)

    for i in range(NUM_SAMPLES):
        # éšæœºé€‰æ‹©ä¸€ç§æ¨¡å¼ï¼š50% å¤šè·³ï¼Œ50% å¯¹æŠ—
        mode = "multihop" if random.random() < 0.5 else "adversarial"
        
        try:
            item_data = {}
            distractor_msgs = []
            
            # å…ˆå¡«å……ä¸€äº›èƒŒæ™¯å™ªéŸ³ä½œä¸ºåº•æ–™
            for _ in range(DISTRACTOR_MSG_COUNT):
                distractor_msgs.append(get_noise(DISTRACTOR_LEN))

            if mode == "multihop":
                # === ç”Ÿæˆå¤šè·³æ•°æ® ===
                chain = prompt_multi | llm | parser_multi
                res = chain.invoke({"format_instructions": parser_multi.get_format_instructions()})
                
                # ç­–ç•¥ï¼šåŸ‹è—ä½ç½®ä¸å˜ï¼Œä½†å»æ‰ã€æ ‡ç­¾ã€‘
                # å¯ä»¥åŠ ä¸€ç‚¹ç‚¹è‡ªç„¶çš„å£è¯­å‰ç¼€ï¼Œè®©å®ƒæ··åœ¨å°è¯´é‡Œä¸é‚£ä¹ˆçªå…€ï¼Œä¹Ÿå¯ä»¥ç›´æ¥æ”¾
                
                idx1, idx2 = 0, len(distractor_msgs) // 2
                
                # ä¿®æ”¹å‰ï¼šdistractor_msgs.insert(idx1, f"ã€çº¿ç´¢Aã€‘{res.fact_1}")
                # ä¿®æ”¹åï¼šç›´æ¥æ”¾å…¥ï¼Œæˆ–è€…åŠ è‡ªç„¶å‰ç¼€
                distractor_msgs.insert(idx1, f"é¡ºä¾¿æä¸€ä¸‹ï¼Œ{res.fact_1}") 
                distractor_msgs.insert(idx2, f"è¿˜æœ‰ä»¶äº‹å¿˜äº†è¯´ï¼Œ{res.fact_2}")
                
                item_data = {
                    "category": "å¤šè·³æ¨ç† (Multi-hop)",
                    "fact_content": f"{res.fact_1} | {res.fact_2}", 
                    "question": res.question,
                    "ground_truth": res.answer
                }

            else:
                # === ç”Ÿæˆå¯¹æŠ—æ•°æ® ===
                chain = prompt_adv | llm | parser_adv
                res = chain.invoke({"format_instructions": parser_adv.get_format_instructions()})
                
                # ç­–ç•¥ï¼šçœŸå®äº‹å®æ”¾åœ¨å¼€å¤´ï¼Œå¯¹æŠ—äº‹å®æ”¾åœ¨ç»“å°¾
                
                # ä¿®æ”¹å‰ï¼šdistractor_msgs.insert(0, f"ã€é‡è¦è®°å½•ã€‘{res.true_fact}")
                # ä¿®æ”¹å‰ï¼šdistractor_msgs.insert(-1, f"ã€é—²èŠå¹²æ‰°ã€‘{res.fake_fact}")

                # ä¿®æ”¹åï¼š
                distractor_msgs.insert(0, f"ä½ éœ€è¦è®°ä½ï¼Œ{res.true_fact}")
                
                # å¯¹æŠ—æ ·æœ¬å¦‚æœä¸åŠ æ ‡ç­¾ï¼Œå°±æ›´å…·è¿·æƒ‘æ€§ï¼
                # æ¯”å¦‚ï¼štrue="å¯†ç æ˜¯1234"ï¼Œfake="ä»¥å‰å¯†ç æ˜¯9999"
                # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼ŒAgent å¿…é¡»ä¾é è¯­ä¹‰çš„æ—¶é—´/çŠ¶æ€åˆ¤æ–­ï¼Œè¿™æ‰æ˜¯çœŸæ­£çš„é«˜éš¾åº¦
                distractor_msgs.insert(-1, f"å“ä¸å¯¹ï¼Œæˆ‘æƒ³èµ·æ¥{res.fake_fact}") 
                
                item_data = {
                    "category": "è¯­ä¹‰å¯¹æŠ— (Adversarial)",
                    "fact_content": res.true_fact,
                    "question": res.question,
                    "ground_truth": res.answer
                }

            # ç»„è£…æœ€ç»ˆ JSON å¯¹è±¡
            # æ³¨æ„ï¼šè¿™é‡Œçš„ç»“æ„å¾®è°ƒäº†ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„ "history" éƒ½æ”¾åœ¨ distractor_messages é‡Œ
            # ä½ çš„ agent.chat éœ€è¦éå†è¿™ä¸ªåˆ—è¡¨å‘é€æ¶ˆæ¯
            final_item = {
                "id": str(uuid.uuid4()),
                "category": item_data["category"],
                "fact": "", # ç•™ç©ºï¼Œå› ä¸ºäº‹å®å·²ç»æ··å…¥ messages äº†
                "distractor_messages": distractor_msgs, 
                "question": item_data["question"],
                "ground_truth": item_data["ground_truth"]
            }
            dataset.append(final_item)

        except Exception as e:
            print(f"  âš ï¸ ç”Ÿæˆå¤±è´¥: {e}")
            continue

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… é«˜éš¾åº¦æ•°æ®é›†å·²ç”Ÿæˆ: {OUTPUT_FILE}")

if __name__ == "__main__":
    generate_dataset_hard()
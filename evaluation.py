import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from pathlib import Path
import time
import json
import uuid
import wandb
import numpy as np
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# å¯¼å…¥LLM agent
from memagent import MemoryAgent, MemoryAgentConfig

# ================= é…ç½® =================
DATASET_FILE = "experiment_dataset.json"
PROJECT_NAME = "LLM-Memory-System-Final"

# ç”¨ä½œè£åˆ¤è¯„åˆ†çš„LLM
eval_llm = ChatOpenAI(
    model="qwen-plus", 
    temperature=0,
    openai_api_key="your_api_key_here",
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# è£åˆ¤æ‰“åˆ†ï¼šè¿”å› True/False å’Œ åˆ†æ•°
def llm_judge(question, answer, truth):
    prompt = f"""
    æ ‡å‡†ç­”æ¡ˆ: {truth}
    AI å›ç­”: {answer}
    é—®é¢˜: {question}
    
    è¯·åˆ¤æ–­ AI å›ç­”æ˜¯å¦åŒ…å«äº†æ ‡å‡†ç­”æ¡ˆçš„æ ¸å¿ƒæ„æ€ã€‚
    è¾“å‡º JSON: {{"correct": true/false, "score": 1-5}}
    """
    try:
        res = eval_llm.invoke(prompt).content
        if "```" in res: res = res.split("```json")[-1].split("```")[0]
        data = json.loads(res)
        return data["correct"], data["score"]
    except:
        return False, 0


# è¿è¡Œè¯„æµ‹å‡½æ•°
def run_evaluation():
    # åŠ è½½æ•°æ®
    with open(DATASET_FILE, "r", encoding="utf-8") as f:
        dataset = json.load(f)
        
    # åˆå§‹åŒ– WandB
    wandb.init(project=PROJECT_NAME, name="Comprehensive-Eval-v1")
    
    # å®šä¹‰ WandB è¡¨æ ¼åˆ—
    columns = [
        "Config", "Category", "Question", "Answer", "Truth", 
        "Correct", "Score", "Latency(s)", 
        "Vector Hit", "Graph Hit"
    ]
    table = wandb.Table(columns=columns)
    
    # å®šä¹‰å¯¹æ¯”å®éªŒç»„æ¶ˆèå®éªŒï¼‰
    configs = [
        {"name": "Baseline (No Mem)", "vec": False, "graph": False},
        {"name": "Vector Only",       "vec": True,  "graph": False},
        {"name": "Graph Only",        "vec": False, "graph": True},
        {"name": "Hybrid (Full)",     "vec": True,  "graph": True},
    ]


    print(f"ğŸš€ å¼€å§‹è¯„æµ‹ï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬ x {len(configs)} ç§é…ç½®")

    # æ‰€æœ‰è¯„æµ‹äº§ç‰©æ”¾è¿™é‡Œ
    BASE_RUN_DIR = Path("./eval_runs")  

    # å¾ªç¯éå†æ‰€æœ‰çš„æµ‹è¯•
    for conf in configs:
        print(f"\n--- Running Configuration: {conf['name']} ---")
        
        metrics = {
            "latency": [], "score": [], "accuracy": [],
            "vector_hit_rate": [], "graph_hit_rate": []
        }

        for i, item in enumerate(dataset):
            thread_id = f"{conf['name'].replace(' ', '_')}_{i}_{uuid.uuid4().hex[:8]}"
            run_dir = BASE_RUN_DIR / thread_id
            run_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºAgenté…ç½®
            config = MemoryAgentConfig(
                verbose=True,  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
                vector_store_path=str(run_dir / "chroma"),              # æ¯æ¬¡ä¸€ä¸ªå…¨æ–°å‘é‡åº“ç›®å½•
                checkpoints_db=str(run_dir / "checkpoints.sqlite")      # æ¯æ¬¡ä¸€ä¸ªå…¨æ–°checkpointåº“
            )
            
            # åˆå§‹åŒ–Agent
            agent = MemoryAgent(config)
            
            # è®°å¿†æ¤å…¥
            if item.get("fact"):
                agent.chat(
                    item["fact"], 
                    thread_id=thread_id,
                    enable_vector=conf["vec"], 
                    enable_graph=conf["graph"]
                )
            
            # å¤šè½®å¹²æ‰° (å…³é”®æ­¥éª¤)
            # è¿™ä¸€æ­¥ä¼šå¤šæ¬¡è°ƒç”¨ Agentï¼Œæ¨¡æ‹Ÿæ—¶é—´æµé€å’Œä¸Šä¸‹æ–‡æ»‘åŠ¨
            for dist_msg in item["distractor_messages"]:
                agent.chat(
                    dist_msg, 
                    thread_id=thread_id, 
                    enable_vector=conf["vec"], 
                    enable_graph=conf["graph"]
                )
                
            # æé—®ä¸æµ‹è¯•
            start_time = time.time()
            
            # è·å– Final State ä»¥æ£€æŸ¥ Context
            final_state = agent.chat(
                item["question"], 
                thread_id=thread_id, 
                enable_vector=conf["vec"], 
                enable_graph=conf["graph"]
            )
            
            end_time = time.time()
            latency = end_time - start_time

            # å…³é—­agent
            agent.close()
            
            # è§£æç»“æœ
            ai_msg = final_state["messages"][-1].content
            print(ai_msg)
            
            # æ£€æŸ¥ State ä¸­çš„ context æ˜¯å¦ä¸ºç©º
            vector_hit = 1 if len(final_state.get("vector_context", "")) > 10 else 0
            graph_hit = 1 if len(final_state.get("graph_context", "")) > 10 else 0
            
            # LLM è£åˆ¤
            is_correct, score = llm_judge(item["question"], ai_msg, item["ground_truth"])
            
            # è®°å½•æ•°æ®
            metrics["latency"].append(latency)
            metrics["score"].append(score)
            metrics["accuracy"].append(1 if is_correct else 0)
            metrics["vector_hit_rate"].append(vector_hit)
            metrics["graph_hit_rate"].append(graph_hit)
            
            # æ·»åŠ åˆ° WandB è¡¨æ ¼
            table.add_data(
                conf["name"], item["category"], item["question"], ai_msg, item["ground_truth"],
                is_correct, score, round(latency, 2), vector_hit, graph_hit
            )
            
            print(f"   [{i+1}/{len(dataset)}] Q: {item['question'][:15]}... | Correct: {is_correct} | V-Hit: {vector_hit} | G-Hit: {graph_hit}")

        # è®¡ç®—è¯¥é…ç½®çš„å¹³å‡æŒ‡æ ‡å¹¶ Log
        wandb.log({
            f"{conf['name']}/avg_latency": np.mean(metrics["latency"]),
            f"{conf['name']}/avg_score": np.mean(metrics["score"]),
            f"{conf['name']}/accuracy": np.mean(metrics["accuracy"]),
            f"{conf['name']}/vector_hit_rate": np.mean(metrics["vector_hit_rate"]),
            f"{conf['name']}/graph_hit_rate": np.mean(metrics["graph_hit_rate"]),
        })

    wandb.log({"Evaluation Details": table})
    wandb.finish()
    print("\næ‰€æœ‰è¯„æµ‹å®Œæˆï¼è¯·å‰å¾€ WandB æŸ¥çœ‹å¯è§†åŒ–æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    run_evaluation()